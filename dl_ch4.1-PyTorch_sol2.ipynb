{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update 03.08.2021\n",
    "\n",
    "#### Plan for tomorrow\n",
    "\n",
    "Continue write up, think I need to explain where predicitons and loss are stored. Might need to review chapter again at some point.\n",
    "\n",
    "### Update 02.08.2021\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Kaggle writeup](https://www.kaggle.com/aakashkhadka/mnist-pytorch)\n",
    "- [PyTorch MNIST Fashion tutorial - Datasets / dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "- [PyTorch MNIST source code](https://pytorch.org/vision/stable/_modules/torchvision/datasets/mnist.html#MNIST)\n",
    "- [PyTorch overview - creating data models](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)\n",
    "\n",
    "#### Plan for tomorrow\n",
    "\n",
    "Examine sources above, make notes, think about how to structure this notebook and the writeup\n",
    "\n",
    "\n",
    "### Update 30.07.2021\n",
    "\n",
    "This version appears to offer the most accurate model so I'll use this as the basis for my blog post.\n",
    "\n",
    "\n",
    "### Update 28.07.2021\n",
    "\n",
    "The files structure is quite messed up here. I'm going to label each of these to make writing up easier. This file is:\n",
    "\n",
    "- external solution using PyTorch and MNIST CSV file\n",
    "- [source](https://www.kaggle.com/aakashkhadka/mnist-pytorch)\n",
    "\n",
    "\n",
    "**Goals**\n",
    "\n",
    "- Reconfiguring my colab project for fastai Chapter 4. \n",
    "- Extending project with PyTorch and fastai features\n",
    "\n",
    "**Update 01.06.2021**\n",
    "\n",
    "I think the key to completing this project (my own code in fact) is to find a way to integrate PyTorch into it. If I just follow the example but adapt it to my own purpose I think I might just be able to train this model.\n",
    "\n",
    "**Update 31.05.2021** \n",
    "\n",
    "Not made much progress today, but I do think it's worth checking out the PyTorch documentation on DataLoaders and Datasets as I think these make it much clearer what the code is doing compared to the Kaggle doc: [link](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "**Update 26.05.2021**\n",
    "\n",
    "I think for my next post I'll discuss how H&G's solution compares to the one I found on Kaggle. From there I can still work on coding up my own bespoke code but at least it gives me a solution to the original blog premise.\n",
    "\n",
    "\n",
    "**Update 19.05.2021**\n",
    "\n",
    "- The model works now, I had just missed out a section. I'm still not sure why it works or for that matter what it's doing, but at least the code is running locally\n",
    "\n",
    "*Next steps*\n",
    "\n",
    "- Compare this code to my previous material and that of H&G. Perhaps this might help me work it out and also enable me to finally complete this project.\n",
    "\n",
    "**Update 18.05.2021**\n",
    "\n",
    "- Defining training and testing data sets before the class definition seems to have created an issue\n",
    "- There appears to be an issue where the data is not being passed to the GPU\n",
    "\n",
    "**Update 13.05.2021**\n",
    "\n",
    "This [Kaggle Project](https://www.kaggle.com/aakashkhadka/mnist-pytorch) appears to have a solution using PyTorch. I will adjust this writeup to incorporate the solution here.\n",
    "\n",
    "#### Notes\n",
    "\n",
    "- Could remove the exploritory stuff since this has already been covered in the last blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog notes\n",
    "\n",
    "In my last update, I was unable to train a model capable of recognising handwritten digits. The reason for this was mainly due to my inability to structure the data (a CSV file based on MNIST database) in such a manner that the mathematical operation necessary to train a model could be performed on it (see below for more details).\n",
    "\n",
    "While I have still have not been able to code up a solution to this problem myself for an update I decided to compare some approaches to this problem. To begin with, we have Howard and Guggen's partial solution (which is only capable of distinguishing 3s and 7s) that uses a different approach where direct greyscale labeled images are used and this project from Kaggle (which I have amended slightly) that uses the same CSV data I have but that trains the model with some features from PyTorch.\n",
    "\n",
    "My intention with this post then is to examine these code bases and highlight some features I think that are useful in terms of training a model to recognisne digits. I may at some point update this post again if I can apply these techniques to my own code directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries required for data analysis and visualisation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "# Dataset and DataLoader are of particular importance as these will \n",
    "# structure the data such that the necessary mathematically operations\n",
    "# required to generate a model can be applied to the data.\n",
    "\n",
    "# For more details review: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    " \n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST data containing number label and values for pixel hue in a\n",
    "# 28x28 grid\n",
    "\n",
    "training_data = open('mnist_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame based on training data\n",
    "\n",
    "training_data_df = pd.read_csv(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample of DateFrame values. Includes label and pixel hue values\n",
    "\n",
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an MNIST Dataset that outputs an array consisting of the \n",
    "# number label and pixel hues for that number. ToTensor converst this\n",
    "# array to a tensor that can be used for processing by the GPU.\n",
    "\n",
    "\n",
    "\n",
    "class MNIST_Dataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        # Create a DataFrame based on MNIST CSV file\n",
    "        num_pix_df = pd.read_csv(path)\n",
    "        # Series of number labels\n",
    "        self.num_labels = num_pix_df['label']\n",
    "        # DF for all pixel values\n",
    "        self.pix_vals = num_pix_df.iloc[:,1:].values # Review how this works in more detail\n",
    "        # Hue value of pixel from integer images\n",
    "        self.n_samples = len(num_pix_df)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        #Tuple pixel values and np array of number labels \n",
    "        sample = self.pix_vals[index],np.array(self.num_labels[index]) \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "# Converts arrays to tensors\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, target=sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Training and Testing sets from my locally stored MNIST csv\n",
    "# files using the MNIST class created above\n",
    "\n",
    "training_path = open('mnist_train.csv')\n",
    "testing_path = open('mnist_test.csv')\n",
    "\n",
    "training_set = MNIST_Dataset(training_path, transform = ToTensor())\n",
    "testing_set = MNIST_Dataset(testing_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f07bc6c3580>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here is a visual example of number from the data set\n",
    "\n",
    "example = training_set[0]\n",
    "plt.imshow(example[0].numpy().reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's in-built DataLoader function to structure batches for\n",
    "# training and testing data sets. Dividing the data in this way \n",
    "# improves GPU performance\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "training_loader=DataLoader(training_set,batch_size=100,shuffle=True)\n",
    "testing_loader=DataLoader(testing_set,batch_size=100,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f07bc779eb0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another visual example, this time a sample from the batch. Since\n",
    "# the data is shuffle it should adjust each time this command is run.\n",
    "\n",
    "plt.imshow(iter(training_loader).next()[0][0].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(training_loader).next()[0][0].dtype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we generate a neural net to perform the matrix operations on the\n",
    "# data. First some presets must be configured\n",
    "\n",
    "\n",
    "# we set this to 1 because the hues are greyscale image\n",
    "input_size=1\n",
    "# first convolution converts 1 channel to 16 channels in feature maps\n",
    "hid1_size=16\n",
    "# then to 32 channels\n",
    "hid2_size=32\n",
    "\n",
    "k_conv_size=5\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that defines the neutral net. This has two layers and the \n",
    "# forward function defines the process between the two layers to \n",
    "# progressively improve the prediction\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1=torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(input_size,hid1_size,k_conv_size),\n",
    "        torch.nn.BatchNorm2d(hid1_size),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=2)\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.layer2=torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(hid1_size,hid2_size,k_conv_size),\n",
    "            torch.nn.BatchNorm2d(hid2_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc=torch.nn.Linear(512,10)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.layer1(x)\n",
    "        \n",
    "        x=self.layer2(x)\n",
    "        # Changing the image into one dimensional tensor for feeding \n",
    "        # the fully connected layers\n",
    "        x=x.reshape(x.shape[0],-1)\n",
    "       \n",
    "        x=self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating the neural net from the GPU you - must be an NVIDIA model\n",
    "\n",
    "model=Net()\n",
    "device=torch.device('cuda')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we generate the loss function, in this case a Cross Entropy Loss\n",
    "\n",
    "lr=1e-3\n",
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=lr,momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the number of epochs for the model to run\n",
    "\n",
    "epochs=3\n",
    "loss_values=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6321, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.5504, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2624, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.2140, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1542, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1304, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1838, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1246, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0821, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1307, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1306, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0744, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1786, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.1781, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0924, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Here the pixel hues are converted back to images. I wonder if there\n",
    "# would be a more direct way to analyse the images?\n",
    "\n",
    "\n",
    "targets=np.array([])\n",
    "preds=np.array([])\n",
    "for epoch in range(epochs):\n",
    "    for i,(img,target) in enumerate(training_loader):\n",
    "        img=img.reshape(100,1,28,28).float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(img)\n",
    "        pred=torch.argmax(output,axis=1)\n",
    "#         print(target,pred)\n",
    "        targets=np.hstack([targets,target.cpu().numpy()])\n",
    "        preds=np.hstack([preds,pred.cpu().numpy()])\n",
    "        loss=loss_fn(output,target.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100==0:\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9506388888888889"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(targets,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.950270950220083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(targets,preds,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
